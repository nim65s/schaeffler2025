{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWisfEp-br4O"
   },
   "source": [
    "# Introduction to the basics of RL algorithms\n",
    "\n",
    "This notebook introduces some basic algorithms, before digging to more serious implementation using GPU-enabled simulators and baseline solver implementation in a second notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWKJJTS5b9H0"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We need torch, gymnasium, and stable-baseline (just for the replay-buffer, I agree it is quite overkill). Plus we need a way to properly display the environments inside the notebook, which is tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import matplotlib.pylab as plt\n",
    "import stable_baselines3 as sb3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering\n",
    "Gymnasium renders by default its environments in a X window. We need a bypass for rendering them in notebooks, using a canvas for pywidget. \n",
    "The solution below pre-load a canvas, and then use it with images generated by the environment. On some setups, it can blink. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1',render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "from ipycanvas import Canvas\n",
    "import time\n",
    "canvas = Canvas(width=400, height=600)\n",
    "display(canvas)\n",
    "\n",
    "for j in range(200):\n",
    "    env.step(j % 2)\n",
    "    # Render in the matplotlib canvas\n",
    "    canvas.put_image_data(env.render(), 0, 0)\n",
    "    time.sleep(1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAFd0I9WbqYl",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Starting simple with a Q-Table\n",
    "\n",
    "We first focus on a full-discrete environment, which allows to work with a Q-table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzu5U4VVgJdJ"
   },
   "source": [
    "### Code of the environment (execute but don't read it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Jy7hMdjcXS-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "class EnvMountainCarFullyDiscrete(gym.Wrapper):\n",
    "    def __init__(self, n_bins=(51, 51),**kwargs):\n",
    "        env = gym.make(\"MountainCar-v0\",**kwargs).unwrapped\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "        # Définition des bornes des états\n",
    "        self.state_bounds = [\n",
    "            [-1.2, 0.6],   # Position du chariot\n",
    "            [-0.1, .1],   # Vitesse du chariot\n",
    "       ]\n",
    "\n",
    "        # Création des bins pour discrétiser l'état\n",
    "        self.bins = [np.linspace(low-1e-6, high+1e-6, num=n, endpoint=True)\n",
    "                     for (low, high), n in zip(self.state_bounds, self.n_bins)]\n",
    "        # Basis value for representing the bin indexes as a single integer\n",
    "        self.bin_base = [ int(np.prod(self.n_bins[:n])) for n in range(len(self.n_bins)) ]\n",
    "\n",
    "        # Définition de l'espace d'observation discret\n",
    "        #self.observation_space = gym.spaces.MultiDiscrete(self.n_bins)\n",
    "        self.observation_space = gym.spaces.Discrete(np.prod(self.n_bins))\n",
    "\n",
    "    def discrete_state_to_index(self,bins):\n",
    "        return sum([ idx*base for (idx,base) in zip(bins,self.bin_base) ])\n",
    "    def index_to_discrete_state(self,index):\n",
    "        bins = []\n",
    "        for n in self.n_bins:\n",
    "            bins.append(index%n)\n",
    "            index = index // n\n",
    "        return bins\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convertit un état continu en un index discret.\"\"\"\n",
    "        return [np.digitize(s, bins) for s, bins in zip(state, self.bins)]\n",
    "\n",
    "    def undiscretize_state(self, discrete_state):\n",
    "        \"\"\"Transforme un état discret en état continu en prenant le centre du bin.\"\"\"\n",
    "        assert self.is_discrete_state_in_range(discrete_state)\n",
    "        return np.array([\n",
    "            (self.bins[i][d-1] + self.bins[i][d]) / 2 if 0 < d < len(self.bins[i]) else self.state_bounds[i][d == 0]\n",
    "            for i, d in enumerate(discrete_state)\n",
    "        ])\n",
    "\n",
    "    def is_discrete_state_in_range(self,discrete_state):\n",
    "        \"\"\"Check that non of the bins is out of the state space\"\"\"\n",
    "        res = 0 not in discrete_state\n",
    "        res = res and  np.all([ d<n for d,n in zip(discrete_state,self.n_bins) ])\n",
    "        return res\n",
    "\n",
    "    def is_index_in_range(self,index):\n",
    "        if index<0 or index>self.observation_space.n: return False\n",
    "        return self.is_discrete_state_in_range(self.index_to_discrete_state(index))\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Exécute une action et retourne l'état sous forme discrète.\"\"\"\n",
    "        # Step 10 times in the continuous space to ensure enough movements\n",
    "        # to pass a discrete bin.\n",
    "        for i in range(10):\n",
    "            next_state_cont, reward, done, trunc, info = self.env.step(action)\n",
    "            if done: break\n",
    "        next_state_discrete = self.discretize_state(next_state_cont)\n",
    "        if not self.is_discrete_state_in_range(next_state_discrete):\n",
    "            done = False\n",
    "            trunc = True\n",
    "            self.state = None\n",
    "        else:\n",
    "            # Reconvertir en état continu pour rester dans un espace propre\n",
    "            self.env.state = self.undiscretize_state(next_state_discrete)\n",
    "            self.discrete_state = next_state_discrete\n",
    "            self.state = self.discrete_state_to_index(next_state_discrete)\n",
    "        return self.state, reward, done, trunc, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Réinitialise l'environnement avec un état strictement discret.\"\"\"\n",
    "        continuous_state, info = self.env.reset(**kwargs)\n",
    "        self.discrete_state = self.discretize_state(continuous_state)\n",
    "        assert self.is_discrete_state_in_range(self.discrete_state)\n",
    "        self.state = self.discrete_state_to_index(self.discrete_state)\n",
    "        # On force le reset sur un état discretisé\n",
    "        self.env.state = self.undiscretize_state(self.discrete_state)\n",
    "        return self.state,info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYMevHMOciA6"
   },
   "source": [
    "### Environment usage\n",
    "You can play with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "6esmHjr2cdJB",
    "outputId": "fe24dfc7-708e-4b74-dc61-87ca70378d09"
   },
   "outputs": [],
   "source": [
    "env = EnvMountainCarFullyDiscrete(render_mode='rgb_array')\n",
    "\n",
    "# Configure the renderer\n",
    "canvas = Canvas(width=400, height=600)\n",
    "display(canvas)\n",
    "\n",
    "env.reset()\n",
    "canvas.put_image_data(env.render(), 0, 0)\n",
    "time.sleep(0.5)\n",
    "\n",
    "for _ in range(20):\n",
    "    # Play the game\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "\n",
    "    # Then render\n",
    "    canvas.put_image_data(env.render(), 0, 0)\n",
    "    time.sleep(1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkaymbtigWeg"
   },
   "source": [
    "### Q-Table algorithm\n",
    "\n",
    "The algorithm represent the Q value as a simple table Q(x,u). The policy is an argmax on the possible u values: policy(x) = argmax(Q[x,:]). The optimization is done by applying the Belman operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qs1iuWsNgWBN",
    "outputId": "c25f9286-0d8e-4198-f7a0-d1c3a5d4348f"
   },
   "outputs": [],
   "source": [
    "### --- Random seed\n",
    "RANDOM_SEED = 1188  # int((time.time()%10)*1000)\n",
    "print(\"Seed = %d\" % RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "### --- Environment\n",
    "env = EnvMountainCarFullyDiscrete()\n",
    "\n",
    "### --- Hyper paramaters\n",
    "NEPISODES = 4000  # Number of training episodes\n",
    "NSTEPS = 50  # Max episode length\n",
    "LEARNING_RATE = 0.85  #\n",
    "DECAY_RATE = 0.99  # Discount factor\n",
    "\n",
    "assert isinstance(env.observation_space,gym.spaces.discrete.Discrete)\n",
    "assert isinstance(env.action_space,gym.spaces.discrete.Discrete)\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])  # Q-table initialized to 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "h_rwd = []  # Learning history (for plot).\n",
    "for episode in range(1, NEPISODES):\n",
    "    x,_ = env.reset()\n",
    "    rsum = 0.0\n",
    "    for steps in range(NSTEPS):\n",
    "        u = np.argmax(\n",
    "            Q[x, :] + np.random.randn(1, env.action_space.n) / episode\n",
    "        )  # Greedy action with noise\n",
    "        x2, reward, done, trunc, info = env.step(u)\n",
    "\n",
    "        # Compute reference Q-value at state x respecting HJB\n",
    "        Qref = reward + DECAY_RATE * np.max(Q[x2, :])\n",
    "\n",
    "        # Update Q-Table to better fit HJB\n",
    "        Q[x, u] += LEARNING_RATE * (Qref - Q[x, u])\n",
    "        x = x2\n",
    "        rsum += reward\n",
    "        if done or trunc: break\n",
    "\n",
    "    h_rwd.append(rsum)\n",
    "    if not episode % 200:\n",
    "        print(\n",
    "           \"Episode #%d done with average cost %.2f\" % (episode, sum(h_rwd[-20:]) / 20)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c-eVpusgxfo"
   },
   "source": [
    "We can plot some results.\n",
    "\n",
    "First, the learning curve, showing cumulative rewards accross episods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "-n4vao2MeRyb",
    "outputId": "9870cccd-2acf-43f5-e4bd-2821db05b1d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ### Plot the learning curve\n",
    "print(\"Total rate of success: %.3f\" % (sum(h_rwd) / NEPISODES))\n",
    "plt.plot(np.cumsum(h_rwd) / range(1, len(h_rwd)+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVjsiPPkhRnd"
   },
   "source": [
    "Then, let's rollout an episod of the resulting policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "2uoU0wGbhGQK",
    "outputId": "7516dd72-d50c-4001-951c-34a6bfa5114e"
   },
   "outputs": [],
   "source": [
    "envrender = EnvMountainCarFullyDiscrete(render_mode = 'rgb_array')\n",
    "canvas = Canvas(width=400, height=600)\n",
    "display(canvas)\n",
    "\n",
    "s,_ = envrender.reset()\n",
    "traj = [s]\n",
    "canvas.put_image_data(envrender.render(), 0, 0)\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(100):\n",
    "    a = np.argmax(Q[s, :])\n",
    "    s, r, done, trunc, info = envrender.step(a)\n",
    "    traj.append(s) # traj is stored to later plot it\n",
    "\n",
    "    if done: break # Will not display the last state, where the car is not visible anymore\n",
    "\n",
    "    canvas.put_image_data(envrender.render(), 0, 0)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYzgyVhAg8d2"
   },
   "source": [
    "Finally, the value function. We display it as a color map along position and velocity of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "xHZyQfB0g69i",
    "outputId": "195309b0-bd34-400f-9085-c0ca0dcddd16"
   },
   "outputs": [],
   "source": [
    "# ### Plot Q-Table as value function pos-vs-vel\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "# Compute the (q,v)\\in R^2 expression of each state\n",
    "def indexes_to_continuous_states(indexes):\n",
    "    return [ env.undiscretize_state(env.index_to_discrete_state(s))\n",
    "             for s in indexes if env.is_index_in_range(s) ]\n",
    "qvs = indexes_to_continuous_states(range(env.observation_space.n))\n",
    "# Compute the value of each state\n",
    "values = [ np.max(Q[s,:]) for s in range(env.observation_space.n)\n",
    "           if env.is_index_in_range(s) ]\n",
    "# Compute the policy of each state\n",
    "policies = [ np.argmax(Q[s,:]) for s in range(env.observation_space.n)\n",
    "           if env.is_index_in_range(s) ]\n",
    "\n",
    "# Plot value on the left, policy and traj on the right ...\n",
    "# ... value\n",
    "vplot = axs[0].scatter([qv[0] for qv in qvs],[qv[1] for qv in qvs],c=values)\n",
    "plt.colorbar(vplot,ax=axs[0])\n",
    "axs[0].set_title('Value function')\n",
    "# ... policy\n",
    "piplot = axs[1].scatter([qv[0] for qv in qvs],[qv[1] for qv in qvs],c=policies)\n",
    "axs[1].set_title('Policy function with traj')\n",
    "plt.colorbar(piplot,ax=axs[1])\n",
    "# ... traj for the displayed rollout.\n",
    "qv_traj = indexes_to_continuous_states(traj)\n",
    "plt.plot([qv[0] for qv in qv_traj],[qv[1] for qv in qv_traj],'r-', linewidth=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUFN_GNWhHWZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Making the table deep\n",
    "\n",
    "Next, we redo the same but considering the table as a simplistic neural network, and enforcing the Belman recursion through gradient descent. Much less efficient, but opens the door to deeper representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iGJLH9Nhh7PI",
    "outputId": "8844257d-5f76-4156-993a-7c9543a62fdb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QValueNetwork(nn.Module):\n",
    "    def __init__(self, env, learning_rate=LEARNING_RATE):\n",
    "        assert isinstance(env.observation_space,gym.spaces.discrete.Discrete)\n",
    "        assert isinstance(env.action_space,gym.spaces.discrete.Discrete)\n",
    "\n",
    "        super(QValueNetwork, self).__init__()\n",
    "\n",
    "        # Linear layer from input size NX to output size NU\n",
    "        self.fc = nn.Linear(env.observation_space.n,env.action_space.n)\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qvalue = self.fc(x)\n",
    "        return qvalue\n",
    "\n",
    "    def predict_action(self, x, noise=0):\n",
    "        qvalue = self.forward(x)\n",
    "        if noise != 0:\n",
    "            qvalue += torch.randn(qvalue.shape) * noise\n",
    "        u = torch.argmax(qvalue,dim=1)\n",
    "        return u\n",
    "\n",
    "    def update(self, x, qref):\n",
    "        self.optimizer.zero_grad()\n",
    "        qvalue = self.forward(x)\n",
    "        loss = F.mse_loss(qvalue, qref)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def plot(self, traj=None):\n",
    "        '''\n",
    "        Plot Q-Table as value function pos-vs-vel\n",
    "        If <traj>  is given as a list of states, then it is also plot in the Pi (right)\n",
    "        subplot.\n",
    "        '''\n",
    "        def indexes_to_continuous_states(indexes):\n",
    "            return [ env.undiscretize_state(env.index_to_discrete_state(s))\n",
    "                     for s in indexes if env.is_index_in_range(s) ]\n",
    "        Q = self.fc.weight.T\n",
    "        fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
    "        qvs = indexes_to_continuous_states(range(env.observation_space.n))\n",
    "        values = [ float(torch.max(Q[s,:])) for s in range(env.observation_space.n)\n",
    "                   if env.is_index_in_range(s) ]\n",
    "        policies = [ int(torch.argmax(Q[s,:])) for s in range(env.observation_space.n)\n",
    "                     if env.is_index_in_range(s) ]\n",
    "        # Plot value on the left, policy and traj on the right ...\n",
    "        # ... value\n",
    "        vplot = axs[0].scatter([qv[0] for qv in qvs],[qv[1] for qv in qvs],c=values)\n",
    "        plt.colorbar(vplot,ax=axs[0])\n",
    "        axs[0].set_title('Value function')\n",
    "        # ... policy\n",
    "        piplot = axs[1].scatter([qv[0] for qv in qvs],[qv[1] for qv in qvs],c=policies)\n",
    "        plt.colorbar(piplot,ax=axs[1])\n",
    "        axs[1].set_title('Policy')\n",
    "        # ... traj for the displayed rollout.\n",
    "        if traj is not None:\n",
    "            qv_traj = indexes_to_continuous_states(traj)\n",
    "            plt.plot([qv[0] for qv in qv_traj],[qv[1] for qv in qv_traj],'r-', linewidth=3)\n",
    "\n",
    "\n",
    "qvalue = QValueNetwork(env, learning_rate=LEARNING_RATE)\n",
    "\n",
    "def one_hot(ix, env):\n",
    "    \"\"\"Return a one-hot encoded tensor.\n",
    "\n",
    "    - ix: index or batch of indices\n",
    "    - n: number of classes (size of the one-hot vector)\n",
    "    \"\"\"\n",
    "    ix = torch.tensor(ix).long()\n",
    "    if ix.dim() == 0:  # If a single index, add batch dimension\n",
    "        ix = ix.unsqueeze(0)\n",
    "    return F.one_hot(ix,num_classes=env.observation_space.n).to(torch.float32)\n",
    "\n",
    "### --- History of search\n",
    "h_rwd = []  # Learning history (for plot).\n",
    "\n",
    "### --- Training\n",
    "for episode in range(1, NEPISODES):\n",
    "    x, _ = env.reset()\n",
    "    rsum = 0.0\n",
    "\n",
    "    for step in range(NSTEPS - 1):\n",
    "\n",
    "        u = int(qvalue.predict_action(one_hot(x,env),noise=1/episode))  # ... with noise\n",
    "        x2, reward, done, trunc, info = env.step(u)\n",
    "\n",
    "        # Compute reference Q-value at state x respecting HJB\n",
    "        # Q2 = sess.run(qvalue.qvalue, feed_dict={qvalue.x: onhot(x2)})\n",
    "        qnext = qvalue(one_hot(x2,env))\n",
    "        qref = qvalue(one_hot(x,env))\n",
    "        qref[0, u] = reward + DECAY_RATE * torch.max(qnext)\n",
    "\n",
    "        # Update Q-table to better fit HJB\n",
    "        #sess.run(qvalue.optim, feed_dict={qvalue.x: onehot(x), qvalue.qref: Qref})\n",
    "        qvalue.update(x=one_hot(x,env),qref=qref)\n",
    "\n",
    "        rsum += reward\n",
    "        x = x2\n",
    "        if done or trunc: break\n",
    "\n",
    "    h_rwd.append(rsum)\n",
    "    if not episode % 200:\n",
    "        print(\"Episode #%d done with %d sucess\" % (episode, sum(h_rwd[-20:])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gtZPaqWii3o"
   },
   "source": [
    "Similarly, we can plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "gWVMJ-LIieTO",
    "outputId": "645ada50-34ca-4478-9f70-0330469d8618"
   },
   "outputs": [],
   "source": [
    "print(\"Total rate of success: %.3f\" % (sum(h_rwd) / NEPISODES))\n",
    "plt.plot(np.cumsum(h_rwd) / range(1, len(h_rwd)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "OfIDBJbAiv79",
    "outputId": "1545a37b-dbc9-4a7e-c9ac-cb582a1f8c48"
   },
   "outputs": [],
   "source": [
    "envrender = EnvMountainCarFullyDiscrete(render_mode = 'rgb_array')\n",
    "canvas = Canvas(width=400, height=600)\n",
    "display(canvas)\n",
    "\n",
    "s,_ = envrender.reset()\n",
    "traj = [s]\n",
    "\n",
    "canvas.put_image_data(envrender.render(), 0, 0)\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(100):\n",
    "    a = int(qvalue.predict_action(one_hot(s,env)))\n",
    "    s, r, done, trunc, info = envrender.step(a)\n",
    "    traj.append(s)\n",
    "\n",
    "    if done: break\n",
    "        \n",
    "    canvas.put_image_data(envrender.render(), 0, 0)\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "H_Q4AfyIi8PL",
    "outputId": "cbc6d979-bff2-4981-83e5-59848d5abb49"
   },
   "outputs": [],
   "source": [
    "qvalue.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mseYIhUdi9lj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DQN\n",
    "\n",
    "Now that we can accept a neural representation of the Q function, any inputs can be taken. Let's switch to a continuous state. For now, we will keep the argmax policy decision, meaning we need to stay in an environment with discrete policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t_U2JdlTjRce",
    "outputId": "7e955f0a-9c03-44bb-825b-fa601584a07f"
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "# HYPERPARAM\n",
    "SEED = 1\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "NUM_ENVS = 1\n",
    "LEARNING_RATE = 0.00025\n",
    "BUFFER_SIZE = 10000\n",
    "TOTAL_TIMESTEPS = 500000\n",
    "TRAIN_FREQUENCY = 10\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TARGET_NETWORK_FREQUENCY = 500\n",
    "TAU = 1.0\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# env setup\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(ENV_ID, SEED + i, i, False, \"TP11_DQN\") for i in range(NUM_ENVS)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "q_network = QNetwork(envs)\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "target_network = QNetwork(envs)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    BUFFER_SIZE,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    \"cpu\",\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "\n",
    "# start the game\n",
    "obs, _ = envs.reset(seed=SEED)\n",
    "started = np.zeros(NUM_ENVS, dtype=bool)\n",
    "h_rwd = []\n",
    "\n",
    "for global_step in range(TOTAL_TIMESTEPS):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    epsilon = linear_schedule(1, 0.05, .5 * TOTAL_TIMESTEPS, global_step)\n",
    "    if random.random() < epsilon:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        q_values = q_network(torch.Tensor(obs))\n",
    "        actions = torch.argmax(q_values, dim=1).numpy()\n",
    "\n",
    "    # execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    # record rewards for plotting purposes\n",
    "    if \"episode\" in infos:\n",
    "        for idx in range(NUM_ENVS):\n",
    "            if truncations[idx] or terminations[idx]:\n",
    "                assert infos[\"episode\"][\"l\"]>0\n",
    "                h_rwd.append(infos[\"episode\"][\"r\"][idx])\n",
    "                if len(h_rwd) % 100 == 0:  # verbose one every ~200 episode\n",
    "                    print(f\"global_step={global_step}/{TOTAL_TIMESTEPS},\"\n",
    "                          + f\" episodic_return={h_rwd[-1]}\")\n",
    "                \n",
    "    # Add samples to replay buffer if env not starting\n",
    "    if np.any(started):\n",
    "        rb.add(obs[started], next_obs[started], actions[started],\n",
    "                rewards[started], terminations[started], infos)\n",
    "    obs = next_obs\n",
    "    started = np.logical_not(np.logical_or(terminations, truncations))\n",
    "\n",
    "    # CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > TOTAL_TIMESTEPS // 50 and global_step % TRAIN_FREQUENCY == 0:\n",
    "        data = rb.sample(BATCH_SIZE)\n",
    "        with torch.no_grad():\n",
    "            target_max, _ = target_network(data.next_observations).max(dim=1)\n",
    "            td_target = data.rewards.flatten() + GAMMA * target_max * (1 - data.dones.flatten())\n",
    "        old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "        loss = F.mse_loss(td_target, old_val)\n",
    "\n",
    "        if int(round((global_step/100) ** (1/3))) ** 3 ==  global_step/100:\n",
    "            print(f'*** Learning goes ... loss = {loss},  q = {old_val.mean()}')\n",
    "\n",
    "        # optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # update target network\n",
    "    if global_step % TARGET_NETWORK_FREQUENCY == 0:\n",
    "        for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
    "            target_network_param.data.copy_(\n",
    "                TAU * q_network_param.data + (1.0 - TAU) * target_network_param.data\n",
    "            )\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0h4TQHuIj2y5"
   },
   "source": [
    "And the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "Vxc7CFmwjQ4Y",
    "outputId": "581a54e5-9090-4f1f-d8ce-d4eb7e1010fc"
   },
   "outputs": [],
   "source": [
    "print(f\"Total rate of success: {np.mean(h_rwd)}\")\n",
    "plt.plot(np.cumsum(h_rwd) / range(1, len(h_rwd)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "kHVktc78j5fQ",
    "outputId": "80e5b81b-d6b9-43f1-a0fe-7025e919d714"
   },
   "outputs": [],
   "source": [
    "envrender = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "canvas = Canvas(width=400, height=600)\n",
    "display(canvas)\n",
    "\n",
    "s,_ = envrender.reset()\n",
    "traj = [s]\n",
    "\n",
    "canvas.put_image_data(envrender.render(), 0, 0)\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(100):\n",
    "    a = int(torch.argmax(q_network(torch.Tensor(s).unsqueeze(0))))\n",
    "    s, r, done, trunc, info = envrender.step(a)\n",
    "    traj.append(s)\n",
    "\n",
    "    if done: break\n",
    "        \n",
    "    canvas.put_image_data(envrender.render(), 0, 0)\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoIZUip8kKBv"
   },
   "source": [
    "This environment has a state of dimension 4. We plot the value on the positions only, cuting at zero velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "9Av2kDMJkQ2t",
    "outputId": "b5effe2e-b5fd-4d14-d002-01609087d2c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "hq = []\n",
    "hp = []\n",
    "ha = []\n",
    "for pos in np.arange(envs.single_observation_space.low[0],\n",
    "                     envs.single_observation_space.high[0],.1):\n",
    "    for angle in np.arange(envs.single_observation_space.low[2],\n",
    "                           envs.single_observation_space.high[2],.01):\n",
    "        hp.append(pos)\n",
    "        ha.append(angle)\n",
    "        hq.append(float(torch.max(\n",
    "            q_network(torch.tensor([[pos, 0, angle, 0]], dtype=torch.float32)))))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(hp, ha, c=hq)\n",
    "plt.colorbar(label='Max Q-value')\n",
    "plt.xlabel('Cart Position')\n",
    "plt.ylabel('Pole Angle')\n",
    "plt.title('Q-value landscape (velocity = 0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWdCJbu7kWU-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Deep deterministic policy gradient (DDPG)\n",
    "\n",
    "Finally, let's consider the case where the actions also are living in a continuous space. In that case, we also need to train a policy network to locally optimize the value (i.e. greedy behavior on the value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hoMTistIkm1D",
    "outputId": "66bc32b6-7b4c-421e-851e-31e0b1bf084d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# HYPERPARAM\n",
    "SEED = 1\n",
    "ENV_ID = \"Pendulum-v1\"\n",
    "NUM_ENVS = 1\n",
    "LEARNING_RATE = 3e-4\n",
    "BUFFER_SIZE = int(1e6)\n",
    "TOTAL_TIMESTEPS = 30000\n",
    "TRAIN_FREQUENCY = 2\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "TARGET_NETWORK_FREQUENCY = 2\n",
    "TAU = 0.005\n",
    "EXPLORATION_NOISE = 0.1\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = torch.cat([x, u], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv([make_env(ENV_ID, SEED, 0, False, \"TP11_DDPG\")])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "actor = Actor(envs)\n",
    "qf1 = QNetwork(envs)\n",
    "qf1_target = QNetwork(envs)\n",
    "target_actor = Actor(envs)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "q_optimizer = torch.optim.Adam(list(qf1.parameters()), lr=LEARNING_RATE)\n",
    "actor_optimizer = torch.optim.Adam(list(actor.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "envs.single_observation_space.dtype = np.float32\n",
    "rb = ReplayBuffer(\n",
    "    BUFFER_SIZE,\n",
    "    envs.single_observation_space,\n",
    "    envs.single_action_space,\n",
    "    \"cpu\",\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "\n",
    "# start the game\n",
    "obs, _ = envs.reset(seed=SEED)\n",
    "h_rwd = []\n",
    "\n",
    "for global_step in range(TOTAL_TIMESTEPS):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < TOTAL_TIMESTEPS//40:\n",
    "        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.Tensor(obs))\n",
    "            actions += torch.normal(0, actor.action_scale * EXPLORATION_NOISE)\n",
    "            actions = actions.numpy().clip(envs.single_action_space.low,\n",
    "                                           envs.single_action_space.high)\n",
    "\n",
    "    # execute the game\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    # record rewards for plotting purposes\n",
    "    if \"episode\" in infos:\n",
    "        for idx in range(NUM_ENVS):\n",
    "            if truncations[idx] or terminations[idx]:\n",
    "                assert infos[\"episode\"][\"l\"]>0\n",
    "                h_rwd.append(infos[\"episode\"][\"r\"][idx])\n",
    "                if len(h_rwd) % 20 == 0:  # verbose one every ~20 episode\n",
    "                    print(f\"episode=#{len(h_rwd)} global_step={global_step}/{TOTAL_TIMESTEPS},\"\n",
    "                          + f\" episodic_return={h_rwd[-1]} length={infos['episode']['l'][idx]}\")\n",
    "                \n",
    "    # Add samples to replay buffer if env not starting\n",
    "    if np.any(started):\n",
    "        rb.add(obs[started], next_obs[started], actions[started],\n",
    "                rewards[started], terminations[started], infos)\n",
    "    obs = next_obs\n",
    "    started = np.logical_not(np.logical_or(terminations, truncations))\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > TOTAL_TIMESTEPS // 40:\n",
    "        data = rb.sample(BATCH_SIZE)\n",
    "        with torch.no_grad():\n",
    "            next_state_actions = target_actor(data.next_observations)\n",
    "            qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "            next_q_value = data.rewards.flatten() \\\n",
    "                + (1 - data.dones.flatten()) * GAMMA * (qf1_next_target).view(-1)\n",
    "\n",
    "        qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % TRAIN_FREQUENCY == 0:\n",
    "            actor_loss = -qf1(data.observations, actor(data.observations)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "        # update the target network\n",
    "        if global_step % TARGET_NETWORK_FREQUENCY == 0:\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rate of success: {np.mean(h_rwd)}\")\n",
    "plt.plot(np.cumsum(h_rwd) / range(1, len(h_rwd)+1))\n",
    "plt.title('Learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a roll-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envrender = gym.make(ENV_ID, render_mode = 'rgb_array')\n",
    "canvas = Canvas(width=400, height=600)\n",
    "display(canvas)\n",
    "\n",
    "s,_ = envrender.reset()\n",
    "traj = [s]\n",
    "\n",
    "canvas.put_image_data(envrender.render(), 0, 0)\n",
    "time.sleep(0.5)\n",
    "\n",
    "for i in range(100):\n",
    "    a = actor(torch.Tensor(s))\n",
    "    a = a.detach().numpy()[0].clip(envrender.action_space.low,\n",
    "                                envrender.action_space.high)\n",
    "    s, r, done, trunc, info = envrender.step(a)\n",
    "    traj.append(s)\n",
    "\n",
    "    if done: break\n",
    "        \n",
    "    canvas.put_image_data(envrender.render(), 0, 0)\n",
    "    time.sleep(0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the value and policy landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = []\n",
    "hpol = []\n",
    "hval = []\n",
    "for pos in np.arange(-np.pi,np.pi,.1):\n",
    "    for vel in np.arange(envs.single_observation_space.low[2],\n",
    "                           envs.single_observation_space.high[2],.1):\n",
    "        s = torch.tensor([np.cos(pos), np.sin(pos),vel], dtype=torch.float32)\n",
    "        a = actor(torch.Tensor(s)).clip(torch.tensor(envrender.action_space.low),torch.tensor(envrender.action_space.high)).detach()\n",
    "        val = qf1(s.reshape(1,3),a).detach()\n",
    "        hs.append(s.numpy())\n",
    "        hpol.append(a.numpy())\n",
    "        hval.append(val.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig,axs = plt.subplots(1,2,figsize=(10,5))\n",
    "# .. .value\n",
    "vplot = axs[0].scatter([ np.arctan2(s[1],s[0]) for s in hs ], [ s[2] for s in hs ], c=hval)\n",
    "plt.colorbar(vplot,label='Max Q-value',ax=axs[0])\n",
    "axs[0].set_xlabel('Angle')\n",
    "axs[0].set_ylabel('Vel')\n",
    "axs[0].set_title('Q-value landscape ')\n",
    "# ... policy\n",
    "pyplot = axs[1].scatter([ np.arctan2(s[1],s[0]) for s in hs ], [ s[2] for s in hs ], c=hpol)\n",
    "plt.colorbar(pyplot,label='Policy',ax=axs[1])\n",
    "axs[1].set_xlabel('Angle')\n",
    "axs[1].set_ylabel('Vel')\n",
    "axs[1].set_title('Policy')\n",
    "# ... roll-out\n",
    "axs[0].plot([np.arctan2(qv[1],qv[0]) for qv in traj],[qv[2] for qv in traj],'r-', linewidth=3)\n",
    "axs[1].plot([np.arctan2(qv[1],qv[0]) for qv in traj],[qv[2] for qv in traj],'r-', linewidth=3)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
